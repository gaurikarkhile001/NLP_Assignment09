{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyhNNf7b8_8m"
      },
      "source": [
        "# 9.\n",
        "\n",
        "A) Implement n-gram model.\n",
        "\n",
        "B) Implement Latent Dirichilet Allocation model and Latent Semantic Analysis for topic modelling using text: 'the quick brown fox',\n",
        "       'the slow brown dog',\n",
        "       'the quick red dog',\n",
        "       'the lazy yellow fox '\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUapkff9JW7"
      },
      "source": [
        "9A) Implement N-gram Model (Unigram, Bigram, Trigram)\n",
        "An n-gram model predicts the next word based on the previous n-1 words. Here's how we can implement unigrams, bigrams, and trigrams for the given text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKxdkW58_Ia",
        "outputId": "3db3c134-896d-4388-b17b-b825d7206995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrams: [('the',), ('quick',), ('brown',), ('fox',), ('the',), ('slow',), ('brown',), ('dog',), ('the',), ('quick',), ('red',), ('dog',), ('the',), ('lazy',), ('yellow',), ('fox',)]\n",
            "Bigrams: [('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'the'), ('the', 'slow'), ('slow', 'brown'), ('brown', 'dog'), ('dog', 'the'), ('the', 'quick'), ('quick', 'red'), ('red', 'dog'), ('dog', 'the'), ('the', 'lazy'), ('lazy', 'yellow'), ('yellow', 'fox')]\n",
            "Trigrams: [('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'the'), ('fox', 'the', 'slow'), ('the', 'slow', 'brown'), ('slow', 'brown', 'dog'), ('brown', 'dog', 'the'), ('dog', 'the', 'quick'), ('the', 'quick', 'red'), ('quick', 'red', 'dog'), ('red', 'dog', 'the'), ('dog', 'the', 'lazy'), ('the', 'lazy', 'yellow'), ('lazy', 'yellow', 'fox')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import defaultdict\n",
        "from nltk import ngrams\n",
        "\n",
        "# Download the required data package for word tokenization\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Sample text\n",
        "documents = [\n",
        "    'the quick brown fox',\n",
        "    'the slow brown dog',\n",
        "    'the quick red dog',\n",
        "    'the lazy yellow fox'\n",
        "]\n",
        "\n",
        "# Tokenize and prepare data\n",
        "tokens = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Flatten all tokens for n-grams\n",
        "flat_tokens = [word for sublist in tokens for word in sublist]\n",
        "\n",
        "# Generate and print n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "print(\"Unigrams:\", generate_ngrams(flat_tokens, 1))\n",
        "print(\"Bigrams:\", generate_ngrams(flat_tokens, 2))\n",
        "print(\"Trigrams:\", generate_ngrams(flat_tokens, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrl9q4WF9Rxh"
      },
      "source": [
        "9B) Topic Modeling using LDA and LSA\n",
        "\n",
        "We will use:\n",
        "\n",
        "LDA (Latent Dirichlet Allocation) — probabilistic model.\n",
        "\n",
        "LSA (Latent Semantic Analysis) — SVD-based method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk8IRg8w9Ls6",
        "outputId": "b0f32751-3378-4d6d-a171-a691d2d49df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in g:\\nlp_practical\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in g:\\nlp_practical\\.venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in g:\\nlp_practical\\.venv\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: click in g:\\nlp_practical\\.venv\\lib\\site-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in g:\\nlp_practical\\.venv\\lib\\site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in g:\\nlp_practical\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in g:\\nlp_practical\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in g:\\nlp_practical\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in g:\\nlp_practical\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in g:\\nlp_practical\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in g:\\nlp_practical\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in g:\\nlp_practical\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: colorama in g:\\nlp_practical\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7vM3xME90-H",
        "outputId": "e5136097-e9e4-4435-a05f-b56aaf6475a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in g:\\nlp_practical\\.venv\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in g:\\nlp_practical\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in g:\\nlp_practical\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in g:\\nlp_practical\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in g:\\nlp_practical\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdTF5XU29-Pt",
        "outputId": "cf857061-90ec-4dd2-d804-b8106fa60b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in g:\\nlp_practical\\.venv\\lib\\site-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "Successfully installed numpy-2.2.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'G:\\NLP_Practical\\.venv\\Lib\\site-packages\\~~mpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'G:\\NLP_Practical\\.venv\\Lib\\site-packages\\~~mpy'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Itl50qIH9jl6",
        "outputId": "3a262b6b-b10b-4540-c78a-aa230f289631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- LDA Topics ---\n",
            "[(0,\n",
            "  '0.205*\"fox\" + 0.201*\"brown\" + 0.122*\"yellow\" + 0.122*\"lazy\" + 0.115*\"slow\" '\n",
            "  '+ 0.113*\"quick\" + 0.081*\"dog\" + 0.042*\"red\"'),\n",
            " (1,\n",
            "  '0.255*\"dog\" + 0.207*\"quick\" + 0.188*\"red\" + 0.078*\"slow\" + 0.072*\"brown\" + '\n",
            "  '0.067*\"fox\" + 0.067*\"lazy\" + 0.066*\"yellow\"')]\n",
            "\n",
            "--- LSA Topics ---\n",
            "Topic 0:\n",
            "['brown', 'quick', 'dog', 'fox', 'red']\n",
            "Topic 1:\n",
            "['fox', 'lazy', 'yellow', 'brown', 'quick']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "import pprint\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Input documents\n",
        "docs = [\n",
        "    'the quick brown fox',\n",
        "    'the slow brown dog',\n",
        "    'the quick red dog',\n",
        "    'the lazy yellow fox'\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "texts = [[word for word in doc.lower().split() if word not in stop_words] for doc in docs]\n",
        "\n",
        "# Create dictionary and corpus for gensim LDA\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# LDA Model\n",
        "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "print(\"\\n--- LDA Topics ---\")\n",
        "pprint.pprint(lda_model.print_topics())\n",
        "\n",
        "# LSA Model using Scikit-Learn\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "lsa_model = TruncatedSVD(n_components=2)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(X)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"\\n--- LSA Topics ---\")\n",
        "for i, comp in enumerate(lsa_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)\n",
        "    print(f\"Topic {i}:\")\n",
        "    print([t[0] for t in sorted_terms[:5]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZh-eAq29tw7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt7XMDII-Ggo"
      },
      "source": [
        " Output Summary:\n",
        "\n",
        "N-gram will show word sequences.\n",
        "\n",
        "LDA will group similar topics probabilistically.\n",
        "\n",
        "LSA will show concept clusters via dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-cuEUvn-Hsl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
